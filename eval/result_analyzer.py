"""
result_analyzer.py - Ph√¢n t√≠ch v√† b√¨nh lu·∫≠n k·∫øt qu·∫£ tr√≠ch xu·∫•t

Module n√†y:
- Ph√¢n t√≠ch ph√¢n b·ªë c√°c ƒë·∫∑c tr∆∞ng
- T·∫°o bi·ªÉu ƒë·ªì th·ªëng k√™
- T·∫°o b√°o c√°o b√¨nh lu·∫≠n t·ª± ƒë·ªông
- So s√°nh v·ªõi d·ªØ li·ªáu tham kh·∫£o
"""

import json
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from collections import Counter
from typing import Dict, List
import matplotlib.pyplot as plt
import seaborn as sns


class ResultAnalyzer:
    """Ph√¢n t√≠ch v√† b√¨nh lu·∫≠n k·∫øt qu·∫£ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng"""
    
    def __init__(self, results_file: str):
        """
        Args:
            results_file: ƒê∆∞·ªùng d·∫´n file JSON ch·ª©a k·∫øt qu·∫£ extraction
        """
        self.results_file = results_file
        self.results = self._load_results()
        self.df = self._parse_to_dataframe()
        self.analysis = {}
    
    def _load_results(self) -> List[Dict]:
        """ƒê·ªçc file k·∫øt qu·∫£"""
        with open(self.results_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _parse_to_dataframe(self) -> pd.DataFrame:
        """Chuy·ªÉn k·∫øt qu·∫£ th√†nh DataFrame ƒë·ªÉ ph√¢n t√≠ch"""
        rows = []
        
        for item in self.results:
            row = {
                'image_path': item.get('image_path', ''),
                'folder': os.path.basename(os.path.dirname(item.get('image_path', ''))),
            }
            
            # Extract features
            for feature in ['shape', 'surface', 'aperture_type', 'exine', 'section']:
                feat_dict = item.get(feature, {})
                if isinstance(feat_dict, dict):
                    row[f'{feature}_class'] = feat_dict.get(f'{feature}_class', 
                                                            feat_dict.get('class', None))
                    row[f'{feature}_confidence'] = feat_dict.get('confidence', None)
                else:
                    row[f'{feature}_class'] = feat_dict
            
            # Size
            size_dict = item.get('size', {})
            if isinstance(size_dict, dict):
                row['size_class'] = size_dict.get('size_class', None)
                row['size_value'] = size_dict.get('size_value', None)
                metrics = size_dict.get('metrics', {})
                row['diameter_micron'] = metrics.get('diameter_micron', None)
            
            rows.append(row)
        
        return pd.DataFrame(rows)
    
    def analyze_all(self):
        """Th·ª±c hi·ªán ph√¢n t√≠ch to√†n di·ªán"""
        print("\n" + "="*60)
        print("PH√ÇN T√çCH K·∫æT QU·∫¢ TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG")
        print("="*60)
        
        self.analysis['general'] = self.analyze_general_statistics()
        self.analysis['shape'] = self.analyze_feature_distribution('shape')
        self.analysis['size'] = self.analyze_size_distribution()
        self.analysis['surface'] = self.analyze_feature_distribution('surface')
        self.analysis['aperture'] = self.analyze_feature_distribution('aperture_type')
        self.analysis['exine'] = self.analyze_feature_distribution('exine')
        self.analysis['section'] = self.analyze_feature_distribution('section')
        
        return self.analysis
    
    def analyze_general_statistics(self) -> Dict:
        """Ph√¢n t√≠ch th·ªëng k√™ chung"""
        print("\nüìä TH·ªêNG K√ä T·ªîNG QUAN:")
        
        total_images = len(self.df)
        unique_folders = self.df['folder'].nunique()
        
        stats = {
            'total_images': total_images,
            'unique_folders': unique_folders,
            'images_per_folder': self.df['folder'].value_counts().to_dict()
        }
        
        print(f"  T·ªïng s·ªë ·∫£nh: {total_images}")
        print(f"  S·ªë folder: {unique_folders}")
        
        return stats
    
    def analyze_feature_distribution(self, feature: str) -> Dict:
        """Ph√¢n t√≠ch ph√¢n b·ªë c·ªßa m·ªôt ƒë·∫∑c tr∆∞ng"""
        col_name = f'{feature}_class'
        
        if col_name not in self.df.columns:
            return {'error': f'Feature {feature} not found'}
        
        print(f"\nüìà PH√ÇN B·ªê {feature.upper()}:")
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class (lo·∫°i b·ªè NaN)
        distribution = self.df[col_name].dropna().value_counts()
        
        if len(distribution) == 0:
            print(f"  ‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu")
            return {'error': 'No data available'}
        
        percentages = self.df[col_name].value_counts(normalize=True) * 100
        
        # In ra
        for cls, count in distribution.items():
            pct = percentages[cls]
            print(f"  {cls}: {count} ({pct:.1f}%)")
        
        # Ph√¢n t√≠ch
        most_common = distribution.index[0]
        least_common = distribution.index[-1]
        
        # ƒê·ªô ƒëa d·∫°ng (entropy)
        probs = distribution / distribution.sum()
        entropy = -np.sum(probs * np.log2(probs))
        max_entropy = np.log2(len(distribution))
        diversity_score = entropy / max_entropy if max_entropy > 0 else 0
        
        analysis = {
            'distribution': distribution.to_dict(),
            'percentages': percentages.to_dict(),
            'most_common': most_common,
            'least_common': least_common,
            'diversity_score': diversity_score,
            'num_classes': len(distribution)
        }
        
        print(f"  ‚û§ Ph·ªï bi·∫øn nh·∫•t: {most_common} ({distribution[most_common]} ·∫£nh)")
        print(f"  ‚û§ √çt g·∫∑p nh·∫•t: {least_common} ({distribution[least_common]} ·∫£nh)")
        print(f"  ‚û§ ƒê·ªô ƒëa d·∫°ng: {diversity_score:.2f} (0=ƒë∆°n ƒëi·ªáu, 1=ƒë·ªìng ƒë·ªÅu)")
        
        # Confidence analysis
        conf_col = f'{feature}_confidence'
        if conf_col in self.df.columns:
            avg_conf = self.df[conf_col].mean()
            if not pd.isna(avg_conf):
                print(f"  ‚û§ ƒê·ªô tin c·∫≠y trung b√¨nh: {avg_conf:.2%}")
                analysis['avg_confidence'] = avg_conf
        
        return analysis
    
    def analyze_size_distribution(self) -> Dict:
        """Ph√¢n t√≠ch ph√¢n b·ªë k√≠ch th∆∞·ªõc"""
        print(f"\nüìè PH√ÇN B·ªê K√çCH TH∆Ø·ªöC:")
        
        if 'diameter_micron' not in self.df.columns:
            return {'error': 'Size data not found'}
        
        sizes = self.df['diameter_micron'].dropna()
        
        if len(sizes) == 0:
            return {'error': 'No valid size data'}
        
        stats = {
            'mean': sizes.mean(),
            'median': sizes.median(),
            'std': sizes.std(),
            'min': sizes.min(),
            'max': sizes.max(),
            'q25': sizes.quantile(0.25),
            'q75': sizes.quantile(0.75)
        }
        
        print(f"  Trung b√¨nh: {stats['mean']:.2f} Œºm")
        print(f"  Trung v·ªã: {stats['median']:.2f} Œºm")
        print(f"  ƒê·ªô l·ªách chu·∫©n: {stats['std']:.2f} Œºm")
        print(f"  Kho·∫£ng: {stats['min']:.2f} - {stats['max']:.2f} Œºm")
        print(f"  T·ª© ph√¢n v·ªã: {stats['q25']:.2f} - {stats['q75']:.2f} Œºm")
        
        # Size class distribution
        if 'size_class' in self.df.columns:
            size_class_dist = self.df['size_class'].value_counts()
            print(f"\n  Ph√¢n b·ªë theo nh√≥m k√≠ch th∆∞·ªõc:")
            for cls, count in size_class_dist.items():
                pct = (count / len(self.df)) * 100
                print(f"    {cls}: {count} ({pct:.1f}%)")
            stats['class_distribution'] = size_class_dist.to_dict()
        
        return stats
    
    def generate_comments(self) -> str:
        """T·∫°o b√¨nh lu·∫≠n t·ª± ƒë·ªông v·ªÅ k·∫øt qu·∫£"""
        comments = []
        
        comments.append("# B√åNH LU·∫¨N V√Ä ƒê√ÅNH GI√Å K·∫æT QU·∫¢\n")
        comments.append("## 1. T·ªïng quan\n")
        
        general = self.analysis.get('general', {})
        comments.append(f"H·ªá th·ªëng ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng **{general.get('total_images', 0)}** ·∫£nh ph·∫•n hoa ")
        comments.append(f"t·ª´ **{general.get('unique_folders', 0)}** folder kh√°c nhau.\n")
        
        comments.append("\n## 2. Ph√¢n t√≠ch t·ª´ng ƒë·∫∑c tr∆∞ng\n")
        
        # Shape
        if 'shape' in self.analysis:
            shape = self.analysis['shape']
            comments.append(f"\n### 2.1 H√¨nh d·∫°ng (Shape)\n")
            comments.append(f"- H√¨nh d·∫°ng ph·ªï bi·∫øn nh·∫•t: **{shape.get('most_common', 'N/A')}** ")
            comments.append(f"({shape.get('distribution', {}).get(shape.get('most_common', ''), 0)} ·∫£nh)\n")
            
            diversity = shape.get('diversity_score', 0)
            if diversity > 0.7:
                comments.append(f"- ƒê·ªô ƒëa d·∫°ng h√¨nh d·∫°ng cao ({diversity:.2f}), cho th·∫•y m·∫´u ph·∫•n c√≥ nhi·ªÅu d·∫°ng kh√°c nhau.\n")
            elif diversity < 0.3:
                comments.append(f"- ƒê·ªô ƒëa d·∫°ng h√¨nh d·∫°ng th·∫•p ({diversity:.2f}), h·∫ßu h·∫øt m·∫´u c√≥ h√¨nh d·∫°ng t∆∞∆°ng t·ª±.\n")
        
        # Size
        if 'size' in self.analysis:
            size = self.analysis['size']
            comments.append(f"\n### 2.2 K√≠ch th∆∞·ªõc (Size)\n")
            comments.append(f"- K√≠ch th∆∞·ªõc trung b√¨nh: **{size.get('mean', 0):.2f} Œºm** ")
            comments.append(f"(ƒë·ªô l·ªách chu·∫©n: {size.get('std', 0):.2f} Œºm)\n")
            comments.append(f"- Kho·∫£ng k√≠ch th∆∞·ªõc: {size.get('min', 0):.2f} - {size.get('max', 0):.2f} Œºm\n")
            
            mean_size = size.get('mean', 0)
            if mean_size < 25:
                comments.append("- Ph·∫•n hoa thu·ªôc nh√≥m k√≠ch th∆∞·ªõc nh·ªè (<25 Œºm)\n")
            elif mean_size < 50:
                comments.append("- Ph·∫•n hoa thu·ªôc nh√≥m k√≠ch th∆∞·ªõc trung b√¨nh (25-50 Œºm)\n")
            else:
                comments.append("- Ph·∫•n hoa thu·ªôc nh√≥m k√≠ch th∆∞·ªõc l·ªõn (>50 Œºm)\n")
            
            cv = (size.get('std', 0) / mean_size) * 100 if mean_size > 0 else 0
            if cv > 20:
                comments.append(f"- H·ªá s·ªë bi·∫øn thi√™n cao ({cv:.1f}%), k√≠ch th∆∞·ªõc kh√¥ng ƒë·ªìng nh·∫•t\n")
            elif cv < 10:
                comments.append(f"- H·ªá s·ªë bi·∫øn thi√™n th·∫•p ({cv:.1f}%), k√≠ch th∆∞·ªõc kh√° ƒë·ªìng nh·∫•t\n")
        
        # Surface
        if 'surface' in self.analysis:
            surface = self.analysis['surface']
            comments.append(f"\n### 2.3 B·ªÅ m·∫∑t (Surface)\n")
            comments.append(f"- Lo·∫°i b·ªÅ m·∫∑t ph·ªï bi·∫øn: **{surface.get('most_common', 'N/A')}**\n")
            
            avg_conf = surface.get('avg_confidence')
            if avg_conf:
                if avg_conf > 0.8:
                    comments.append(f"- ƒê·ªô tin c·∫≠y cao ({avg_conf:.2%}), ph√¢n lo·∫°i b·ªÅ m·∫∑t kh√° ch·∫Øc ch·∫Øn\n")
                elif avg_conf < 0.6:
                    comments.append(f"- ƒê·ªô tin c·∫≠y th·∫•p ({avg_conf:.2%}), c·∫ßn ki·ªÉm tra l·∫°i ph∆∞∆°ng ph√°p texture analysis\n")
        
        # Aperture
        if 'aperture' in self.analysis and 'error' not in self.analysis['aperture']:
            aperture = self.analysis['aperture']
            comments.append(f"\n### 2.4 L·ªó m·ªü (Aperture)\n")
            comments.append(f"- Lo·∫°i aperture ph·ªï bi·∫øn: **{aperture.get('most_common', 'N/A')}**\n")
            
            num_classes = aperture.get('num_classes', 0)
            if num_classes == 1:
                comments.append("- T·∫•t c·∫£ m·∫´u c√≥ c√πng ki·ªÉu aperture, cho th·∫•y ƒë·ªìng nh·∫•t v·ªÅ lo√†i\n")
            elif num_classes > 3:
                comments.append(f"- C√≥ {num_classes} ki·ªÉu aperture kh√°c nhau, m·∫´u ph·∫•n ƒëa d·∫°ng\n")
        else:
            comments.append(f"\n### 2.4 L·ªó m·ªü (Aperture)\n")
            comments.append("- ‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu aperture trong k·∫øt qu·∫£\n")
        
        comments.append("\n## 3. K·∫øt lu·∫≠n\n")
        comments.append("\nD·ª±a tr√™n k·∫øt qu·∫£ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng:\n")
        
        # T√≠nh ch·∫•t t·ªïng qu√°t
        shape_div = self.analysis.get('shape', {}).get('diversity_score', 0)
        if shape_div > 0.6:
            comments.append("- M·∫´u ph·∫•n hoa c√≥ ƒë·ªô ƒëa d·∫°ng cao v·ªÅ h√¨nh th√°i\n")
        else:
            comments.append("- M·∫´u ph·∫•n hoa t∆∞∆°ng ƒë·ªëi ƒë·ªìng nh·∫•t v·ªÅ h√¨nh th√°i\n")
        
        # Ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
        total = general.get('total_images', 1)
        if total > 100:
            comments.append("- K√≠ch th∆∞·ªõc dataset ƒë·ªß l·ªõn ƒë·ªÉ ph√¢n t√≠ch th·ªëng k√™ ƒë√°ng tin c·∫≠y\n")
        elif total < 30:
            comments.append("- Dataset nh·ªè, c·∫ßn thu th·∫≠p th√™m d·ªØ li·ªáu ƒë·ªÉ k·∫øt lu·∫≠n ch·∫Øc ch·∫Øn h∆°n\n")
        
        # Khuy·∫øn ngh·ªã
        comments.append("\n## 4. Khuy·∫øn ngh·ªã\n")
        
        if 'surface' in self.analysis:
            avg_conf = self.analysis['surface'].get('avg_confidence', 0)
            if avg_conf and avg_conf < 0.7:
                comments.append("- C·∫£i thi·ªán ph∆∞∆°ng ph√°p texture analysis ƒë·ªÉ tƒÉng ƒë·ªô tin c·∫≠y ph√¢n lo·∫°i b·ªÅ m·∫∑t\n")
        
        if 'size' in self.analysis:
            cv = (self.analysis['size'].get('std', 0) / self.analysis['size'].get('mean', 1)) * 100
            if cv > 25:
                comments.append("- Ki·ªÉm tra l·∫°i quy tr√¨nh ƒëo k√≠ch th∆∞·ªõc do bi·∫øn thi√™n cao\n")
        
        comments.append("- N√™n th·ª±c hi·ªán ƒë√°nh gi√° ƒë·ªãnh l∆∞·ª£ng v·ªõi ground truth ƒë·ªÉ x√°c minh ƒë·ªô ch√≠nh x√°c\n")
        comments.append("- Xem x√©t ph√¢n t√≠ch theo t·ª´ng folder/lo√†i ƒë·ªÉ c√≥ k·∫øt lu·∫≠n chi ti·∫øt h∆°n\n")
        
        return "".join(comments)
    
    def create_visualizations(self, output_dir: str = 'output'):
        """T·∫°o c√°c bi·ªÉu ƒë·ªì tr·ª±c quan"""
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nüìä ƒêang t·∫°o bi·ªÉu ƒë·ªì...")
        
        # Set style
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (12, 8)
        
        # 1. Shape distribution
        if 'shape_class' in self.df.columns:
            plt.figure(figsize=(10, 6))
            self.df['shape_class'].value_counts().plot(kind='bar', color='steelblue')
            plt.title('Ph√¢n b·ªë H√¨nh d·∫°ng (Shape)', fontsize=14, fontweight='bold')
            plt.xlabel('Lo·∫°i h√¨nh d·∫°ng', fontsize=12)
            plt.ylabel('S·ªë l∆∞·ª£ng', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'shape_distribution.png'), dpi=300)
            plt.close()
            print(f"  ‚úì ƒê√£ l∆∞u: shape_distribution.png")
        
        # 2. Size distribution (histogram)
        if 'diameter_micron' in self.df.columns:
            plt.figure(figsize=(10, 6))
            sizes = self.df['diameter_micron'].dropna()
            plt.hist(sizes, bins=30, color='coral', edgecolor='black', alpha=0.7)
            plt.axvline(sizes.mean(), color='red', linestyle='--', linewidth=2, label=f'Trung b√¨nh: {sizes.mean():.1f} Œºm')
            plt.axvline(sizes.median(), color='green', linestyle='--', linewidth=2, label=f'Trung v·ªã: {sizes.median():.1f} Œºm')
            plt.title('Ph√¢n b·ªë K√≠ch th∆∞·ªõc (Size)', fontsize=14, fontweight='bold')
            plt.xlabel('ƒê∆∞·ªùng k√≠nh (Œºm)', fontsize=12)
            plt.ylabel('T·∫ßn s·ªë', fontsize=12)
            plt.legend()
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'size_distribution.png'), dpi=300)
            plt.close()
            print(f"  ‚úì ƒê√£ l∆∞u: size_distribution.png")
        
        # 3. Surface distribution
        if 'surface_class' in self.df.columns:
            plt.figure(figsize=(10, 6))
            self.df['surface_class'].value_counts().plot(kind='barh', color='forestgreen')
            plt.title('Ph√¢n b·ªë B·ªÅ m·∫∑t (Surface)', fontsize=14, fontweight='bold')
            plt.xlabel('S·ªë l∆∞·ª£ng', fontsize=12)
            plt.ylabel('Lo·∫°i b·ªÅ m·∫∑t', fontsize=12)
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'surface_distribution.png'), dpi=300)
            plt.close()
            print(f"  ‚úì ƒê√£ l∆∞u: surface_distribution.png")
        
        # 4. Multi-feature comparison (pie charts)
        features = ['aperture_type', 'exine', 'section']
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for idx, feature in enumerate(features):
            col = f'{feature}_class'
            if col in self.df.columns:
                data = self.df[col].value_counts()
                axes[idx].pie(data, labels=data.index, autopct='%1.1f%%', startangle=90)
                axes[idx].set_title(feature.replace('_', ' ').title(), fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'multi_feature_pie.png'), dpi=300)
        plt.close()
        print(f"  ‚úì ƒê√£ l∆∞u: multi_feature_pie.png")
        
        # 5. Size by class (boxplot)
        if 'diameter_micron' in self.df.columns and 'shape_class' in self.df.columns:
            plt.figure(figsize=(12, 6))
            self.df.boxplot(column='diameter_micron', by='shape_class', ax=plt.gca())
            plt.title('K√≠ch th∆∞·ªõc theo H√¨nh d·∫°ng', fontsize=14, fontweight='bold')
            plt.suptitle('')  # Remove auto title
            plt.xlabel('Lo·∫°i h√¨nh d·∫°ng', fontsize=12)
            plt.ylabel('ƒê∆∞·ªùng k√≠nh (Œºm)', fontsize=12)
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(os.path.join(output_dir, 'size_by_shape.png'), dpi=300)
            plt.close()
            print(f"  ‚úì ƒê√£ l∆∞u: size_by_shape.png")
        
        print(f"\n‚úÖ ƒê√£ t·∫°o t·∫•t c·∫£ bi·ªÉu ƒë·ªì t·∫°i: {output_dir}/")
    
    def export_analysis_report(self, output_file: str = 'output/analysis_report.md'):
        """Xu·∫•t b√°o c√°o ph√¢n t√≠ch ra file Markdown"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        comments = self.generate_comments()
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(comments)
        
        print(f"\n‚úÖ ƒê√£ l∆∞u b√°o c√°o ph√¢n t√≠ch t·∫°i: {output_file}")


def main():
    """H√†m main"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Ph√¢n t√≠ch v√† b√¨nh lu·∫≠n k·∫øt qu·∫£ tr√≠ch xu·∫•t',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
V√≠ d·ª•:
  python eval/result_analyzer.py --input output/extraction_results.json --output output/analysis_report.md
        '''
    )
    
    parser.add_argument('--input', '-i', required=True, 
                       help='File JSON ch·ª©a k·∫øt qu·∫£ extraction')
    parser.add_argument('--output', '-o', default='output/analysis_report.md',
                       help='File Markdown output cho b√°o c√°o')
    parser.add_argument('--plot-dir', default='output',
                       help='Th∆∞ m·ª•c l∆∞u c√°c bi·ªÉu ƒë·ªì')
    parser.add_argument('--no-plots', action='store_true',
                       help='Kh√¥ng t·∫°o bi·ªÉu ƒë·ªì')
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("PH√ÇN T√çCH V√Ä B√åNH LU·∫¨N K·∫æT QU·∫¢")
    print("="*60)
    
    # Ph√¢n t√≠ch
    analyzer = ResultAnalyzer(args.input)
    analyzer.analyze_all()
    
    # T·∫°o bi·ªÉu ƒë·ªì
    if not args.no_plots:
        analyzer.create_visualizations(args.plot_dir)
    
    # Xu·∫•t b√°o c√°o
    analyzer.export_analysis_report(args.output)
    
    print("\n" + "="*60)
    print("HO√ÄN TH√ÄNH")
    print("="*60)


if __name__ == '__main__':
    main()
